![Profile views](https://komarev.com/ghpvc/?username=yuki-2025&label=Profile%20views) [![GitHub followers](https://img.shields.io/github/followers/yuki-2025?label=Followers&logo=github)](https://github.com/yuki-2025?tab=followers) ![Total Stars](https://img.shields.io/badge/dynamic/json?url=https://api.github-star-counter.workers.dev/user/yuki-2025&query=%24.stars&label=Stars&logo=github)


ðŸ¤— Hi, Iâ€™m @yuki-2025
-
Iâ€™m a seasoned LLM research engineer, ML engineer/data scientist, and AI product manager with training in Applied Data Science at the University of Chicago. I specialize in LLMs, large multimodal models, and AI agents. I bridge academia and industryâ€”as Senior Staff at an AI company and as an AI researcher at UChicago Booth and the Data Science Instituteâ€”turning peer-reviewed research into production systems and leading end-to-end AI implementations.

ðŸ’» Expertise: AI Research & Large Language Models (LLM) ðŸ¤–â€¢ Large multimodal models (LMMs) ðŸŽµ â€¢ Machine Learning & Deep Learning ðŸ“š â€¢ Full-Stack GenAI Applications ðŸ’¡ â€¢ AI Agents ðŸ§ 

My AI Projects (All Open Sourced in github):
-
1. [Paper: AgentNet: Dynamically Graph Structure Selection for LLM-Based Multi-Agent System](https://github.com/yuki-2025/Dyna_Swarm). </br>
   We introduce a dynamic, input-driven **multi-agent system (MAS)**, AgentNet, that executes over learned communication graphs (COT,TOT,GOT). we use Advantage Actorâ€“Critic (A2C) *reinforcement learning* to learn a stable distribution over edges, then we fine-tune the base LLM (LoRA) as a graph selector (LLM-as-judge) that picks the best topology per input. The approach delivers **SOTA** on structured reasoning (Crossword, Game-of-24, MMLU, BBH) and code generation (HumanEval) while keeping latency comparable to CoT/ToT-style and static-swarm baselines. (Paper Still under review in EMNLP).
2. [Paper: Medinotes: A Gen AI Framework for Medical Note Generation](https://github.com/yuki-2025/MediNotes).  </br>
   MediNotes is a **first-gen GenAI framework** that enhances clinical consultations by automating documentation and providing a **healthcare-domainâ€“fine-tuned copilot** with retrieval-augmented generation (RAG), LLM and ambient listening. We co-developed and validated the system with clinicians at UChicago Medicine, culminating in **2 IEEE publications** and pilot deployments in live clinical environments.
3. [Paper: IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning](https://github.com/thqiu0419/IntentVCNet/tree/main)
   We fine-tuned InternVL and LLaMA-Factory, earning second place in the IntentVC Challenge at ACM MM 2025 (Intention-Oriented Controllable Video Captioning), which resulted in a published ACM MM paper.
4. mRAG: Multimodal RAG  + Evaluation 
5. Project cooeprated with University of Chicago Booth School of Business, developed LLM training strategies to generate business-domain embeddings enriched with broad general knowledgeâ€”enabling the extraction of CEO-level actionable insights in management, financial decision-making. 
6. More other project and paper. 
 
<!---
yuki-2025/yuki-2025 is a âœ¨ special âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
